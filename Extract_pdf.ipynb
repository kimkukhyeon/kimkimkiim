{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Extract_pdf.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOxlw5oi0p6DmK7WH62E2TM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimkukhyeon/kimkimkiim/blob/master/Extract_pdf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **프로그래밍 기초 기말고사_20200011_김국현**\n",
        "                                                  "
      ],
      "metadata": {
        "id": "XeAX9i_C_pjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 구글 드라이브 마운트\n",
        "import os, sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "my_path = '/content/notebooks'\n",
        "# Colab Notebooks 안에 my_env 폴더에 패키지 저장\n",
        "os.symlink('/content/drive/My Drive/Colab Notebooks/my_env', my_path)\n",
        "sys.path.insert(0, my_path)"
      ],
      "metadata": {
        "id": "ZzwZst8bFR5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 라이브러리 설치\n",
        "!pip install PyPDF2 PyMuPDF Pillow image tika"
      ],
      "metadata": {
        "id": "3Cd7o57-E0b3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pdf 파일에서 모든 텍스트를 추출하여 txt 파일 형식으로 저장하기\n",
        "from PyPDF2 import PdfFileReader, PdfFileWriter\n",
        "import os\n",
        "\n",
        "path = '/content/drive/MyDrive/Colab Notebooks/KBSI연구과제/' # 디렉토리 형식 결정\n",
        "file_list = os.listdir(path)\n",
        "file_list_py = [file for file in file_list if file.endswith('.pdf')] # 파일 형식 결정\n",
        "\n",
        "n = len(file_list_py) \n",
        "i = 0\n",
        "list_path = []\n",
        "while i < n:\n",
        "  list_path.append('/content/drive/MyDrive/Colab Notebooks/KBSI연구과제/')\n",
        "  i+=1\n",
        "\n",
        "pdfdata = [x+y for x,y in zip(list_path, file_list_py)]\n",
        "\n",
        "for i in pdfdata:\n",
        "  file_path = i\n",
        "  pdf = PdfFileReader(file_path)\n",
        "  file_deposit = i.replace('pdf', 'txt') # 저장할 파일 형식 결정\n",
        "  with open(file_deposit, 'w') as f:\n",
        "    for page_num in range(pdf.numPages):\n",
        "        pageObj = pdf.getPage(page_num)\n",
        "        try: \n",
        "            txt = pageObj.extractText()\n",
        "            print(''.center(100, '-'))\n",
        "        except:\n",
        "            pass\n",
        "        else:\n",
        "            f.write('Page {0}\\n'.format(page_num+1))\n",
        "            f.write(''.center(100, '-'))\n",
        "            f.write(txt)\n",
        "    f.close()"
      ],
      "metadata": {
        "id": "vbYAJY_m-0qe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 라이브러리 사용\n",
        "from tika import parser\n",
        "from PyPDF2 import PdfFileReader, PdfFileWriter\n",
        "import os\n",
        "import fitz\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "import sys\n",
        "\n",
        "\n",
        "# 사용자 모듈\n",
        "def listToString(str_list):\n",
        "    result = \"\"\n",
        "    for s in str_list:\n",
        "        result += s + ' '\n",
        "    return result.strip()\n",
        "\n",
        "def entx_listToString(str_list):\n",
        "    result = \"\"\n",
        "    for s in str_list:\n",
        "        result += s\n",
        "    return result.strip()\n",
        "\n",
        "def listToString_p(str_list):\n",
        "    result = \"\"\n",
        "    for s in str_list:\n",
        "        result += s + \", \"\n",
        "    return result.strip()\n",
        "\n",
        "def printdate(date):\n",
        "  publication_date = date.replace(' ', '.') + '.'\n",
        "  print('(2) Publication Date: ', end = '')\n",
        "  print(publication_date)\n",
        "\n",
        "def sen_capitalize():\n",
        "  for i in abs_list:\n",
        "    a = i.replace('.', '')\n",
        "    b = a.capitalize() + '.'\n",
        "    print(b, end = ' ')\n",
        "  print()\n",
        "\n",
        "year = ['2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021']\n",
        "num = [0, 0, 0, 0, 0, 0, 0, 0]\n",
        "\n",
        "\n",
        "\n",
        "# 구글 드라이브에서 논문 차례대로 가져오기\n",
        "path = '/content/drive/MyDrive/Colab Notebooks/KBSI연구과제/' # 디렉토리 형식 결정\n",
        "file_list = os.listdir(path)\n",
        "file_list_py = [file for file in file_list if file.endswith('.pdf')] # 파일 형식 결정\n",
        "\n",
        "n = len(file_list_py) \n",
        "i = 0\n",
        "list_path = []\n",
        "while i < n:\n",
        "  list_path.append('/content/drive/MyDrive/Colab Notebooks/KBSI연구과제/')\n",
        "  i+=1\n",
        "\n",
        "pdfdata = [x+y for x,y in zip(list_path, file_list_py)]\n",
        "\n",
        "\n",
        "print('<PDF 확장자를 가진 논문 파일에서 데이터 추출 및 정보 수집>_20200011_김국현')\n",
        "print('\\n')\n",
        "\n",
        "print('1. 특정 텍스트 및 사진 추출하기')\n",
        "print()\n",
        "\n",
        "n = 1\n",
        "for i in pdfdata:\n",
        "  raw = parser.from_file(i)\n",
        "  content = raw['content'].strip()\n",
        "  data = content.lower()\n",
        "  list_content = content.split('\\n')\n",
        "  l_list_content = content.lower().split('\\n')\n",
        "  print('[', n, '번째 논문]', sep = '' )\n",
        "  n += 1\n",
        "\n",
        "\n",
        "\n",
        "# (1) 제목(title) 추출하기\n",
        "  if 'doi:' in l_list_content[0]:\n",
        "    list_title = l_list_content[4:6]\n",
        "    title_result = entx_listToString(list_title)\n",
        "    title = title_result.capitalize()\n",
        "    print('(1) Title: ' + title + '.')\n",
        "  elif '&' in l_list_content[0]:\n",
        "    title_start = 9\n",
        "    title_stop = l_list_content.index('', title_start)\n",
        "    list_title = l_list_content[title_start:title_stop]\n",
        "    title_result = listToString(list_title).replace(';', ',')\n",
        "    title = title_result.capitalize()\n",
        "    print('(1) Title: ' + title + '.')\n",
        "  else:\n",
        "    title = list_content[0]\n",
        "    print('(1) Title: ' + title)\n",
        "    \n",
        "    \n",
        "    \n",
        "# (2) 발행연도(publication_date) 추출하기\n",
        "  if 'available online xxx' in data:\n",
        "    list_date = [s for s in l_list_content if \"accepted\" in s][0]\n",
        "    date = list_date.replace(\"accepted\", '').strip()\n",
        "    publication_date = date.replace(' ', '.') + '.'\n",
        "    print('(2) Publication Date: ', end = '')\n",
        "    print(publication_date)\n",
        "    year0 = year.index(publication_date[-5:-1])\n",
        "    num[year0] += 1\n",
        "  elif 'published:' in data:\n",
        "    list_date = [s for s in l_list_content if \"published:\" in s]\n",
        "    date_result = listToString(list_date).split(':')\n",
        "    date = date_result[-1].strip()\n",
        "    publication_date = date.replace(' ', '.') + '.'\n",
        "    print('(2) Publication Date: ', end = '')\n",
        "    print(publication_date)\n",
        "    year0 = year.index(publication_date[-5:-1])\n",
        "    num[year0] += 1\n",
        "  elif 'published online:' in data:\n",
        "    list_date = [s for s in l_list_content if \"published online:\" in s][0]\n",
        "    date = list_date.replace(\"published online:\", '').strip()[0:-1]\n",
        "    publication_date = date.replace(' ', '.') + '.'\n",
        "    print('(2) Publication Date: ', end = '')\n",
        "    print(publication_date)\n",
        "    year0 = year.index(publication_date[-5:-1])\n",
        "    num[year0] += 1\n",
        "  elif 'available online' in data:\n",
        "    if 'available online at' in data:\n",
        "      list_date = [s for s in l_list_content if \"available online\" in s][1]\n",
        "      date = list_date.replace(\"available online\", '').strip()\n",
        "      publication_date = date.replace(' ', '.') + '.'\n",
        "      print('(2) Publication Date: ', end = '')\n",
        "      print(publication_date)\n",
        "      year0 = year.index(publication_date[-5:-1])\n",
        "      num[year0] += 1\n",
        "    else:\n",
        "      list_date = [s for s in l_list_content if \"available online\" in s][0]\n",
        "      date = list_date.replace(\"available online\", '').strip()\n",
        "      publication_date = date.replace(' ', '.') + '.'\n",
        "      print('(2) Publication Date: ', end = '')\n",
        "      print(publication_date)\n",
        "      year0 = year.index(publication_date[-5:-1])\n",
        "      num[year0] += 1\n",
        "  elif 'accepted:' in data:\n",
        "    list_date = [s for s in l_list_content if \"accepted:\" in s][0]\n",
        "    date_result = entx_listToString(list_date).split(':')\n",
        "    date = date_result[-1].strip()\n",
        "    publication_date = date.replace(' ', '.') + '.'\n",
        "    print('(2) Publication Date: ', end = '')\n",
        "    print(publication_date)\n",
        "    year0 = year.index(publication_date[-5:-1])\n",
        "    num[year0] += 1\n",
        "\n",
        "\n",
        "\n",
        "# (3) 저자(author) 추출하기\n",
        "  try:\n",
        "    f_title = list_content[0].split(' ')\n",
        "    f1 = f_title[0].lower()\n",
        "    matching = [s for s in l_list_content if f1 in s]\n",
        "    title2_start = l_list_content.index(matching[1])\n",
        "    title2_start = l_list_content.index(matching[1], 1)\n",
        "    title2_stop = l_list_content.index('', title2_start)\n",
        "    author_data = l_list_content[title2_stop - 1]\n",
        "    if 'doi:' in l_list_content[title2_start - 1]:\n",
        "      author_data = l_list_content[title2_start + 1]\n",
        "      author_data1 = author_data.replace('1 |', ',')\n",
        "      author = author_data1[0:-1].strip() + '.'\n",
        "      print('(3) Author: ', end = '')\n",
        "      print(author)\n",
        "    elif 'doi:' in l_list_content[0]:\n",
        "      author_data = l_list_content[6]\n",
        "      author = author_data.replace('1', '').replace('2', '').replace(',', '').replace('*', '').replace('  and', ',').strip() + '.'\n",
        "      print('(3) Author: ', end = '')\n",
        "      print(author)\n",
        "    elif '&' in l_list_content[0]:\n",
        "      author_index = title_stop + 1\n",
        "      author = l_list_content[author_index].replace(' &', ',') + '.'\n",
        "      print('(3) Author: ', end = '')\n",
        "      print(author)\n",
        "    elif 'doi:' in l_list_content[title2_start - 1]:\n",
        "      f_title = list_content[0].split(' ')\n",
        "      f1 = f_title[0].lower()\n",
        "      matching = [s for s in l_list_content if f1 in s]\n",
        "      title2_start = l_list_content.index(matching[1], 1)\n",
        "      author_data = l_list_content[title2_start + 1]\n",
        "      author_data1 = author_data.replace('1 |', ',')\n",
        "      author = author_data1[0:-1].strip() + '.'\n",
        "      print('(3) Author: ', end = '')\n",
        "      print(author)\n",
        "    else:\n",
        "      f_title = list_content[0].split(' ')\n",
        "      f1 = f_title[0].lower()\n",
        "      matching = [s for s in l_list_content if f1 in s]\n",
        "      title2_start = l_list_content.index(matching[1])\n",
        "      title2_stop = l_list_content.index('', title2_start)\n",
        "      author_data = l_list_content[title2_stop + 1]\n",
        "      if not(',' in l_list_content[title2_stop + 1]):\n",
        "        author_data1 = author_data.replace(' &', ',')\n",
        "        author = author_data1.strip() + '.'\n",
        "        print('(3) Author: ', end = '')\n",
        "        print(author)\n",
        "      else:\n",
        "        author0 = author_data[0:-2]\n",
        "        author11 = author0.replace(' b,', '').replace(' 2', '').replace('b', '').replace(' 1 and', ',').replace(' 1 ', '').replace(' 2', '')\n",
        "        author2 =  author11.replace(' 2 and', ',').replace(' 2 ', '').replace(' b,', ',').replace(' *,', ',')\n",
        "        author3 = author2.replace(' a,', ',').replace(' 1,', ',').replace('a,', ',').replace(' 1', '').replace(' and', ',')\n",
        "        if '(' and ')' in author3:\n",
        "          f_title = list_content[0].split(' ')\n",
        "          f1 = f_title[0].lower()\n",
        "          matching = [s for s in l_list_content if f1 in s]\n",
        "          title2_start = l_list_content.index(matching[1])\n",
        "          title2_stop = l_list_content.index('', title2_start)\n",
        "          author_data = l_list_content[title2_stop - 1]\n",
        "          author_data1 = author_data.replace('a', ',').replace('b,', ',').replace(',', '')\n",
        "          author = author_data1[0:-1].strip() + '.'\n",
        "          print('(3) Author: ', end = '')\n",
        "          print(author)\n",
        "        else:\n",
        "          author = author3.strip() + '.'\n",
        "          print('(3) Author: ', end = '')\n",
        "          print(author)\n",
        "  except:\n",
        "    if '&' in l_list_content[0]:\n",
        "      title_start = 9\n",
        "      title_stop = l_list_content.index('', title_start)\n",
        "      author_index = title_stop + 1\n",
        "      author = l_list_content[author_index].replace(' &', ',') + '.'\n",
        "      print('(3) Author: ', end = '')\n",
        "      print(author)\n",
        "\n",
        "\n",
        "\n",
        "# (4) 키워드(keywords) 추출하기\n",
        "  if 'npp'in l_list_content:\n",
        "    key_start = l_list_content.index('keywords:')\n",
        "    key_stop = l_list_content.index('npp', key_start)\n",
        "    list_key = l_list_content[key_start+1:key_stop]\n",
        "    key_result = listToString_p(list_key).replace(';', ',')\n",
        "    keywords = key_result[:-1] + '.'\n",
        "    print('(4) Keywords: ' + keywords)\n",
        "  elif 'keywords:' in l_list_content:\n",
        "    key_start = l_list_content.index('keywords:')\n",
        "    if '* corresponding author.' in l_list_content:\n",
        "      key_stop = l_list_content.index('* corresponding author.', key_start)\n",
        "      list_key = l_list_content[key_start+1:key_stop]\n",
        "      key_result = listToString(list_key).replace('  ', ', ')\n",
        "      if ',' in key_result:\n",
        "        keywords = key_result[:-1] + '.'\n",
        "        print('(4) Keywords: ' + keywords)\n",
        "      else:\n",
        "        key_result = listToString_p(list_key).replace(';', ',')\n",
        "        keywords = key_result[:-1] + '.'\n",
        "        print('(4) Keywords: ' + keywords)\n",
        "    elif 'e-mail address: skroh@kaeri.re.kr.' in l_list_content:\n",
        "        key_stop = l_list_content.index('e-mail address: skroh@kaeri.re.kr.', key_start)\n",
        "        list_key = l_list_content[key_start+1:key_stop]\n",
        "        key_result = listToString_p(list_key).replace(';', ',')\n",
        "        keywords = key_result[:-1] + '.'\n",
        "        print('(4) Keywords: ' + keywords)\n",
        "    else:\n",
        "      key_stop = l_list_content.index('', key_start)\n",
        "      list_key = l_list_content[key_start+1:key_stop]\n",
        "      key_result = listToString_p(list_key).replace(';', ',')\n",
        "      keywords = key_result[:-1] + '.'\n",
        "      print('(4) Keywords: ' + keywords)\n",
        "  elif 'keywords' in l_list_content:\n",
        "    key_start = l_list_content.index('keywords')\n",
        "    if l_list_content[key_start+1] == '':\n",
        "      key_stop = l_list_content.index('1 | introduction', key_start)\n",
        "      list_key = l_list_content[key_start+2:key_stop]\n",
        "      key_result = listToString_p(list_key).replace(';', ',')\n",
        "      keywords = key_result[:-1] + '.'\n",
        "      print('(4) Keywords: ' + keywords)\n",
        "    else:\n",
        "      key_stop = l_list_content.index('', key_start)\n",
        "      list_key = l_list_content[key_start+1:key_stop]\n",
        "      key_result = listToString(list_key).replace(';', ',')\n",
        "      keywords = key_result[:-1] + '.'\n",
        "      print('(4) Keywords: ' + keywords)\n",
        "  elif 'keywords:' not in l_list_content:\n",
        "    matching = [s for s in l_list_content if \"keywords:\" in s] \n",
        "    for i in matching:\n",
        "      key_start = l_list_content.index(i)\n",
        "      key_stop = l_list_content.index('', key_start)\n",
        "      list_key = l_list_content[key_start:key_stop]\n",
        "      key_result = listToString(list_key).replace(';', ',')\n",
        "      keywords = key_result[10:] + '.'\n",
        "      print('(4) Keywords: ' + keywords)\n",
        "  elif 'keywords' not in l_list_content:\n",
        "    matching = [s for s in l_list_content if \"keywords\" in s] \n",
        "    for i in matching:\n",
        "      key_start = l_list_content.index(i)\n",
        "      key_stop = l_list_content.index('', key_start)\n",
        "      list_key = l_list_content[key_start:key_stop]\n",
        "      key_result = listToString(list_key).replace(';', ',')\n",
        "      keywords = key_result[10:] + '.'\n",
        "      print('(4) Keywords: ' + keywords)\n",
        "\n",
        "\n",
        "\n",
        "# (5) 초록(abstract) 추출하기\n",
        "  if 'summary' in l_list_content:\n",
        "    abs_start = l_list_content.index('summary')\n",
        "    abs_stop = l_list_content.index('keywords', abs_start)\n",
        "    list_abs = l_list_content[abs_start+1:abs_stop]\n",
        "    abs_result = listToString(list_abs).replace('-  ', '').replace('  ', ' ')\n",
        "    abs_list = abs_result.split('. ')\n",
        "    print('(5) Abstract: ', end = '')\n",
        "    abstract = sen_capitalize()\n",
        "  elif 'abstract' in l_list_content:\n",
        "    abs_start = l_list_content.index('abstract')\n",
        "    abs_stop = l_list_content.index('', abs_start)\n",
        "    list_abs = l_list_content[abs_start+1:abs_stop]\n",
        "    abs_result = listToString(list_abs).replace('- ', '')\n",
        "    abs_list = abs_result.split('. ')\n",
        "    print('(5) Abstract: ', end = '')\n",
        "    abstract = sen_capitalize()\n",
        "  elif 'a b s t r a c t' in l_list_content:\n",
        "    if '© 2017 korean nuclear society, published by elsevier korea llc. this is an open access' in l_list_content:\n",
        "      abs_start = l_list_content.index('a b s t r a c t')\n",
        "      abs_stop = l_list_content.index('© 2017 korean nuclear society, published by elsevier korea llc. this is an open access', abs_start)\n",
        "      list_abs = l_list_content[abs_start+1:abs_stop]\n",
        "      abs_result = listToString(list_abs).replace('-  ', '').replace('  ', ' ')\n",
        "      abs_list = abs_result.split('. ')\n",
        "      print('(5) Abstract: ', end = '')\n",
        "      abstract = sen_capitalize()\n",
        "    elif '© 2017 korean nuclear society, published by elsevier korea llc. this is an open access article under the' in l_list_content:\n",
        "      abs_start = l_list_content.index('a b s t r a c t') + 2\n",
        "      abs_stop = l_list_content.index('© 2017 korean nuclear society, published by elsevier korea llc. this is an open access article under the', abs_start)\n",
        "      list_abs = l_list_content[abs_start:abs_stop]\n",
        "      abs_result = listToString(list_abs).replace('- ', '')\n",
        "      abs_list = abs_result.split('. ')\n",
        "      print('(5) Abstract: ', end = '')\n",
        "      abstract = sen_capitalize()\n",
        "    elif '© 2018 korean nuclear society, published by elsevier korea llc. this is an open access article under the' in l_list_content:\n",
        "      abs_start = l_list_content.index('a b s t r a c t')\n",
        "      abs_stop = l_list_content.index('© 2018 korean nuclear society, published by elsevier korea llc. this is an open access article under the', abs_start)\n",
        "      list_abs = l_list_content[abs_start+1:abs_stop]\n",
        "      abs_result = listToString(list_abs).replace('- ', '').replace('0 ', '\\'')\n",
        "      abs_list = abs_result.split('. ')\n",
        "      print('(5) Abstract: ', end = '')\n",
        "      abstract = sen_capitalize()\n",
        "    elif '© 2022 korean nuclear society, published by elsevier korea llc. this is an open access article under the' in l_list_content:\n",
        "      abs_start = l_list_content.index('a b s t r a c t')\n",
        "      abs_stop = l_list_content.index('© 2022 korean nuclear society, published by elsevier korea llc. this is an open access article under the', abs_start)\n",
        "      list_abs = l_list_content[abs_start+1:abs_stop]\n",
        "      abs_result = listToString(list_abs).replace('- ', '').replace('0 ', '\\'')\n",
        "      abs_list = abs_result.split('. ')\n",
        "      print('(5) Abstract: ', end = '')\n",
        "      abstract = sen_capitalize()\n",
        "    else:\n",
        "      abs_start = l_list_content.index('a b s t r a c t')\n",
        "      abs_stop = l_list_content.index('', abs_start)\n",
        "      abs_stop2 = l_list_content.index('', abs_stop+1)\n",
        "      abs_stop3 = l_list_content.index('', abs_stop2+1)\n",
        "      if '1. introduction' == l_list_content[abs_stop3 -1]:\n",
        "        list_abs = list_content[abs_start+1:abs_stop2]\n",
        "        abs_result = listToString(list_abs).replace('- ', '').replace('0 ', '\\'')\n",
        "        abs_list = abs_result.split('. ')\n",
        "        print('(5) Abstract: ', end = '')\n",
        "        abstract = sen_capitalize()\n",
        "      else: \n",
        "        list_abs = l_list_content[abs_start+1:abs_stop3]\n",
        "        abs_result = listToString(list_abs).replace('- ', '').replace('0 ', '\\'').replace('0', '\\'')\n",
        "        abs_list = abs_result.split('. ')\n",
        "        print('(5) Abstract: ', end = '')\n",
        "        abstract = sen_capitalize()\n",
        "  elif 'abstract' not in l_list_content:\n",
        "    matching = [s for s in l_list_content if \"abstract\" in s]\n",
        "    for a in matching:\n",
        "      abs_start = l_list_content.index(a)\n",
        "      abs_stop = l_list_content.index('', abs_start)\n",
        "      list_abs = l_list_content[abs_start:abs_stop]\n",
        "      abs_result = listToString(list_abs).replace('- ', '')\n",
        "      abs = abs_result[10:]\n",
        "      abs_cap = abs_result[10:].capitalize()\n",
        "      if abs[0] == abs_cap[0]:\n",
        "        abs_list = abs_result.split('. ')\n",
        "        print('(5) Abstract: ', end = '')\n",
        "        abstract = sen_capitalize()\n",
        "      else:\n",
        "        abs_list = abs_result[10:].split('. ')\n",
        "        print('(5) Abstract: ', end = '')\n",
        "        abstract = sen_capitalize()\n",
        "  print()\n",
        "\n",
        "\n",
        "# (6) 년도별 논문 발행 빈도분포표 시각화\n",
        "print('2. 논문 작성 수 추세의 시각화')\n",
        "print()\n",
        "\n",
        "font1 = {'size':18, 'color':'blue'}\n",
        "bar = plt.bar(year, num)\n",
        "plt.ylim(0, 8)\n",
        "plt.plot(year, num, color = 'red', linestyle = '-', marker = 'o')\n",
        "for rect in bar:\n",
        "    height = rect.get_height()\n",
        "    plt.text(rect.get_x() + rect.get_width()/2.0, height, '%.1f' % height, ha='center', va='bottom', size = 12)\n",
        "plt.title('The number of SCI papers by year', fontdict=font1)\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Number')\n",
        "\n",
        "plt.savefig('/content/drive/MyDrive/Colab Notebooks/KBSI연구과제/annualpapers_visual.png', facecolor='#eeeeee', bbox_inches='tight', pad_inches=0.5, dpi = 200)\n",
        "plt.show()\n",
        "print()"
      ],
      "metadata": {
        "id": "IlIuciLYzSBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 출력결과 txt 파일로 저장하기\n",
        "from tika import parser\n",
        "from PyPDF2 import PdfFileReader, PdfFileWriter\n",
        "import os\n",
        "import fitz\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "import sys\n",
        "\n",
        "filename = '/content/drive/MyDrive/Colab Notebooks/KBSI연구과제/final_output.txt'\n",
        "sys.stdout = open(filename, 'w')\n",
        "\n",
        "def listToString(str_list):\n",
        "    result = \"\"\n",
        "    for s in str_list:\n",
        "        result += s + ' '\n",
        "    return result.strip()\n",
        "\n",
        "def entx_listToString(str_list):\n",
        "    result = \"\"\n",
        "    for s in str_list:\n",
        "        result += s\n",
        "    return result.strip()\n",
        "\n",
        "def listToString_p(str_list):\n",
        "    result = \"\"\n",
        "    for s in str_list:\n",
        "        result += s + \", \"\n",
        "    return result.strip()\n",
        "\n",
        "def printdate(date):\n",
        "  publication_date = date.replace(' ', '.') + '.'\n",
        "  print('(2) Publication Date: ', end = '')\n",
        "  print(publication_date)\n",
        "\n",
        "def sen_capitalize():\n",
        "  for i in abs_list:\n",
        "    a = i.replace('.', '')\n",
        "    b = a.capitalize() + '.'\n",
        "    print(b, end = ' ')\n",
        "  print()\n",
        "\n",
        "\n",
        "path = '/content/drive/MyDrive/Colab Notebooks/KBSI연구과제/'\n",
        "file_list = os.listdir(path)\n",
        "file_list_py = [file for file in file_list if file.endswith('.pdf')]\n",
        "\n",
        "n = len(file_list_py) \n",
        "i = 0\n",
        "list_path = []\n",
        "while i < n:\n",
        "  list_path.append('/content/drive/MyDrive/Colab Notebooks/KBSI연구과제/')\n",
        "  i+=1\n",
        "\n",
        "pdfdata = [x+y for x,y in zip(list_path, file_list_py)]\n",
        "\n",
        "\n",
        "print('<PDF 확장자를 가진 논문 파일에서 데이터 추출 및 정보 수집>_20200011_김국현')\n",
        "print('\\n')\n",
        "\n",
        "print('1. 특정 텍스트 및 사진 추출하기')\n",
        "print()\n",
        "\n",
        "n = 1\n",
        "for i in pdfdata:\n",
        "  raw = parser.from_file(i)\n",
        "  content = raw['content'].strip()\n",
        "  data = content.lower()\n",
        "  list_content = content.split('\\n')\n",
        "  l_list_content = content.lower().split('\\n')\n",
        "  print('[', n, '번째 논문]', sep = '' )\n",
        "  n += 1\n",
        "\n",
        "  if 'doi:' in l_list_content[0]:\n",
        "    list_title = l_list_content[4:6]\n",
        "    title_result = entx_listToString(list_title)\n",
        "    title = title_result.capitalize()\n",
        "    print('(1) Title: ' + title + '.')\n",
        "  elif '&' in l_list_content[0]:\n",
        "    title_start = 9\n",
        "    title_stop = l_list_content.index('', title_start)\n",
        "    list_title = l_list_content[title_start:title_stop]\n",
        "    title_result = listToString(list_title).replace(';', ',')\n",
        "    title = title_result.capitalize()\n",
        "    print('(1) Title: ' + title + '.')\n",
        "  else:\n",
        "    title = list_content[0]\n",
        "    print('(1) Title: ' + title)\n",
        "    \n",
        "\n",
        "  if 'available online xxx' in data:\n",
        "    list_date = [s for s in l_list_content if \"accepted\" in s][0]\n",
        "    date = list_date.replace(\"accepted\", '').strip()\n",
        "    publication_date = date.replace(' ', '.') + '.'\n",
        "    print('(2) Publication Date: ', end = '')\n",
        "    print(publication_date)\n",
        "    year0 = year.index(publication_date[-5:-1])\n",
        "    num[year0] += 1\n",
        "  elif 'published:' in data:\n",
        "    list_date = [s for s in l_list_content if \"published:\" in s]\n",
        "    date_result = listToString(list_date).split(':')\n",
        "    date = date_result[-1].strip()\n",
        "    publication_date = date.replace(' ', '.') + '.'\n",
        "    print('(2) Publication Date: ', end = '')\n",
        "    print(publication_date)\n",
        "    year0 = year.index(publication_date[-5:-1])\n",
        "    num[year0] += 1\n",
        "  elif 'published online:' in data:\n",
        "    list_date = [s for s in l_list_content if \"published online:\" in s][0]\n",
        "    date = list_date.replace(\"published online:\", '').strip()[0:-1]\n",
        "    publication_date = date.replace(' ', '.') + '.'\n",
        "    print('(2) Publication Date: ', end = '')\n",
        "    print(publication_date)\n",
        "    year0 = year.index(publication_date[-5:-1])\n",
        "    num[year0] += 1\n",
        "  elif 'available online' in data:\n",
        "    if 'available online at' in data:\n",
        "      list_date = [s for s in l_list_content if \"available online\" in s][1]\n",
        "      date = list_date.replace(\"available online\", '').strip()\n",
        "      publication_date = date.replace(' ', '.') + '.'\n",
        "      print('(2) Publication Date: ', end = '')\n",
        "      print(publication_date)\n",
        "      year0 = year.index(publication_date[-5:-1])\n",
        "      num[year0] += 1\n",
        "    else:\n",
        "      list_date = [s for s in l_list_content if \"available online\" in s][0]\n",
        "      date = list_date.replace(\"available online\", '').strip()\n",
        "      publication_date = date.replace(' ', '.') + '.'\n",
        "      print('(2) Publication Date: ', end = '')\n",
        "      print(publication_date)\n",
        "      year0 = year.index(publication_date[-5:-1])\n",
        "      num[year0] += 1\n",
        "  elif 'accepted:' in data:\n",
        "    list_date = [s for s in l_list_content if \"accepted:\" in s][0]\n",
        "    date_result = entx_listToString(list_date).split(':')\n",
        "    date = date_result[-1].strip()\n",
        "    publication_date = date.replace(' ', '.') + '.'\n",
        "    print('(2) Publication Date: ', end = '')\n",
        "    print(publication_date)\n",
        "    year0 = year.index(publication_date[-5:-1])\n",
        "    num[year0] += 1\n",
        "\n",
        "\n",
        "  try:\n",
        "    f_title = list_content[0].split(' ')\n",
        "    f1 = f_title[0].lower()\n",
        "    matching = [s for s in l_list_content if f1 in s]\n",
        "    title2_start = l_list_content.index(matching[1])\n",
        "    title2_start = l_list_content.index(matching[1], 1)\n",
        "    title2_stop = l_list_content.index('', title2_start)\n",
        "    author_data = l_list_content[title2_stop - 1]\n",
        "    if 'doi:' in l_list_content[title2_start - 1]:\n",
        "      author_data = l_list_content[title2_start + 1]\n",
        "      author_data1 = author_data.replace('1 |', ',')\n",
        "      author = author_data1[0:-1].strip() + '.'\n",
        "      print('(3) Author: ', end = '')\n",
        "      print(author)\n",
        "    elif 'doi:' in l_list_content[0]:\n",
        "      author_data = l_list_content[6]\n",
        "      author = author_data.replace('1', '').replace('2', '').replace(',', '').replace('*', '').replace('  and', ',').strip() + '.'\n",
        "      print('(3) Author: ', end = '')\n",
        "      print(author)\n",
        "    elif '&' in l_list_content[0]:\n",
        "      author_index = title_stop + 1\n",
        "      author = l_list_content[author_index].replace(' &', ',') + '.'\n",
        "      print('(3) Author: ', end = '')\n",
        "      print(author)\n",
        "    elif 'doi:' in l_list_content[title2_start - 1]:\n",
        "      f_title = list_content[0].split(' ')\n",
        "      f1 = f_title[0].lower()\n",
        "      matching = [s for s in l_list_content if f1 in s]\n",
        "      title2_start = l_list_content.index(matching[1], 1)\n",
        "      author_data = l_list_content[title2_start + 1]\n",
        "      author_data1 = author_data.replace('1 |', ',')\n",
        "      author = author_data1[0:-1].strip() + '.'\n",
        "      print('(3) Author: ', end = '')\n",
        "      print(author)\n",
        "    else:\n",
        "      f_title = list_content[0].split(' ')\n",
        "      f1 = f_title[0].lower()\n",
        "      matching = [s for s in l_list_content if f1 in s]\n",
        "      title2_start = l_list_content.index(matching[1])\n",
        "      title2_stop = l_list_content.index('', title2_start)\n",
        "      author_data = l_list_content[title2_stop + 1]\n",
        "      if not(',' in l_list_content[title2_stop + 1]):\n",
        "        author_data1 = author_data.replace(' &', ',')\n",
        "        author = author_data1.strip() + '.'\n",
        "        print('(3) Author: ', end = '')\n",
        "        print(author)\n",
        "      else:\n",
        "        author0 = author_data[0:-2]\n",
        "        author11 = author0.replace(' b,', '').replace(' 2', '').replace('b', '').replace(' 1 and', ',').replace(' 1 ', '').replace(' 2', '')\n",
        "        author2 =  author11.replace(' 2 and', ',').replace(' 2 ', '').replace(' b,', ',').replace(' *,', ',')\n",
        "        author3 = author2.replace(' a,', ',').replace(' 1,', ',').replace('a,', ',').replace(' 1', '').replace(' and', ',')\n",
        "        if '(' and ')' in author3:\n",
        "          f_title = list_content[0].split(' ')\n",
        "          f1 = f_title[0].lower()\n",
        "          matching = [s for s in l_list_content if f1 in s]\n",
        "          title2_start = l_list_content.index(matching[1])\n",
        "          title2_stop = l_list_content.index('', title2_start)\n",
        "          author_data = l_list_content[title2_stop - 1]\n",
        "          author_data1 = author_data.replace('a', ',').replace('b,', ',').replace(',', '')\n",
        "          author = author_data1[0:-1].strip() + '.'\n",
        "          print('(3) Author: ', end = '')\n",
        "          print(author)\n",
        "        else:\n",
        "          author = author3.strip() + '.'\n",
        "          print('(3) Author: ', end = '')\n",
        "          print(author)\n",
        "  except:\n",
        "    if '&' in l_list_content[0]:\n",
        "      title_start = 9\n",
        "      title_stop = l_list_content.index('', title_start)\n",
        "      author_index = title_stop + 1\n",
        "      author = l_list_content[author_index].replace(' &', ',') + '.'\n",
        "      print('(3) Author: ', end = '')\n",
        "      print(author)\n",
        "\n",
        "\n",
        "  if 'npp'in l_list_content:\n",
        "    key_start = l_list_content.index('keywords:')\n",
        "    key_stop = l_list_content.index('npp', key_start)\n",
        "    list_key = l_list_content[key_start+1:key_stop]\n",
        "    key_result = listToString_p(list_key).replace(';', ',')\n",
        "    keywords = key_result[:-1] + '.'\n",
        "    print('(4) Keywords: ' + keywords)\n",
        "  elif 'keywords:' in l_list_content:\n",
        "    key_start = l_list_content.index('keywords:')\n",
        "    if '* corresponding author.' in l_list_content:\n",
        "      key_stop = l_list_content.index('* corresponding author.', key_start)\n",
        "      list_key = l_list_content[key_start+1:key_stop]\n",
        "      key_result = listToString(list_key).replace('  ', ', ')\n",
        "      if ',' in key_result:\n",
        "        keywords = key_result[:-1] + '.'\n",
        "        print('(4) Keywords: ' + keywords)\n",
        "      else:\n",
        "        key_result = listToString_p(list_key).replace(';', ',')\n",
        "        keywords = key_result[:-1] + '.'\n",
        "        print('(4) Keywords: ' + keywords)\n",
        "    elif 'e-mail address: skroh@kaeri.re.kr.' in l_list_content:\n",
        "        key_stop = l_list_content.index('e-mail address: skroh@kaeri.re.kr.', key_start)\n",
        "        list_key = l_list_content[key_start+1:key_stop]\n",
        "        key_result = listToString_p(list_key).replace(';', ',')\n",
        "        keywords = key_result[:-1] + '.'\n",
        "        print('(4) Keywords: ' + keywords)\n",
        "    else:\n",
        "      key_stop = l_list_content.index('', key_start)\n",
        "      list_key = l_list_content[key_start+1:key_stop]\n",
        "      key_result = listToString_p(list_key).replace(';', ',')\n",
        "      keywords = key_result[:-1] + '.'\n",
        "      print('(4) Keywords: ' + keywords)\n",
        "  elif 'keywords' in l_list_content:\n",
        "    key_start = l_list_content.index('keywords')\n",
        "    if l_list_content[key_start+1] == '':\n",
        "      key_stop = l_list_content.index('1 | introduction', key_start)\n",
        "      list_key = l_list_content[key_start+2:key_stop]\n",
        "      key_result = listToString_p(list_key).replace(';', ',')\n",
        "      keywords = key_result[:-1] + '.'\n",
        "      print('(4) Keywords: ' + keywords)\n",
        "    else:\n",
        "      key_stop = l_list_content.index('', key_start)\n",
        "      list_key = l_list_content[key_start+1:key_stop]\n",
        "      key_result = listToString(list_key).replace(';', ',')\n",
        "      keywords = key_result[:-1] + '.'\n",
        "      print('(4) Keywords: ' + keywords)\n",
        "  elif 'keywords:' not in l_list_content:\n",
        "    matching = [s for s in l_list_content if \"keywords:\" in s] \n",
        "    for i in matching:\n",
        "      key_start = l_list_content.index(i)\n",
        "      key_stop = l_list_content.index('', key_start)\n",
        "      list_key = l_list_content[key_start:key_stop]\n",
        "      key_result = listToString(list_key).replace(';', ',')\n",
        "      keywords = key_result[10:] + '.'\n",
        "      print('(4) Keywords: ' + keywords)\n",
        "  elif 'keywords' not in l_list_content:\n",
        "    matching = [s for s in l_list_content if \"keywords\" in s] \n",
        "    for i in matching:\n",
        "      key_start = l_list_content.index(i)\n",
        "      key_stop = l_list_content.index('', key_start)\n",
        "      list_key = l_list_content[key_start:key_stop]\n",
        "      key_result = listToString(list_key).replace(';', ',')\n",
        "      keywords = key_result[10:] + '.'\n",
        "      print('(4) Keywords: ' + keywords)\n",
        "\n",
        "\n",
        "  if 'summary' in l_list_content:\n",
        "    abs_start = l_list_content.index('summary')\n",
        "    abs_stop = l_list_content.index('keywords', abs_start)\n",
        "    list_abs = l_list_content[abs_start+1:abs_stop]\n",
        "    abs_result = listToString(list_abs).replace('-  ', '').replace('  ', ' ')\n",
        "    abs_list = abs_result.split('. ')\n",
        "    print('(5) Abstract: ', end = '')\n",
        "    abstract = sen_capitalize()\n",
        "  elif 'abstract' in l_list_content:\n",
        "    abs_start = l_list_content.index('abstract')\n",
        "    abs_stop = l_list_content.index('', abs_start)\n",
        "    list_abs = l_list_content[abs_start+1:abs_stop]\n",
        "    abs_result = listToString(list_abs).replace('- ', '')\n",
        "    abs_list = abs_result.split('. ')\n",
        "    print('(5) Abstract: ', end = '')\n",
        "    abstract = sen_capitalize()\n",
        "  elif 'a b s t r a c t' in l_list_content:\n",
        "    if '© 2017 korean nuclear society, published by elsevier korea llc. this is an open access' in l_list_content:\n",
        "      abs_start = l_list_content.index('a b s t r a c t')\n",
        "      abs_stop = l_list_content.index('© 2017 korean nuclear society, published by elsevier korea llc. this is an open access', abs_start)\n",
        "      list_abs = l_list_content[abs_start+1:abs_stop]\n",
        "      abs_result = listToString(list_abs).replace('-  ', '').replace('  ', ' ')\n",
        "      abs_list = abs_result.split('. ')\n",
        "      print('(5) Abstract: ', end = '')\n",
        "      abstract = sen_capitalize()\n",
        "    elif '© 2017 korean nuclear society, published by elsevier korea llc. this is an open access article under the' in l_list_content:\n",
        "      abs_start = l_list_content.index('a b s t r a c t') + 2\n",
        "      abs_stop = l_list_content.index('© 2017 korean nuclear society, published by elsevier korea llc. this is an open access article under the', abs_start)\n",
        "      list_abs = l_list_content[abs_start:abs_stop]\n",
        "      abs_result = listToString(list_abs).replace('- ', '')\n",
        "      abs_list = abs_result.split('. ')\n",
        "      print('(5) Abstract: ', end = '')\n",
        "      abstract = sen_capitalize()\n",
        "    elif '© 2018 korean nuclear society, published by elsevier korea llc. this is an open access article under the' in l_list_content:\n",
        "      abs_start = l_list_content.index('a b s t r a c t')\n",
        "      abs_stop = l_list_content.index('© 2018 korean nuclear society, published by elsevier korea llc. this is an open access article under the', abs_start)\n",
        "      list_abs = l_list_content[abs_start+1:abs_stop]\n",
        "      abs_result = listToString(list_abs).replace('- ', '').replace('0 ', '\\'')\n",
        "      abs_list = abs_result.split('. ')\n",
        "      print('(5) Abstract: ', end = '')\n",
        "      abstract = sen_capitalize()\n",
        "    elif '© 2022 korean nuclear society, published by elsevier korea llc. this is an open access article under the' in l_list_content:\n",
        "      abs_start = l_list_content.index('a b s t r a c t')\n",
        "      abs_stop = l_list_content.index('© 2022 korean nuclear society, published by elsevier korea llc. this is an open access article under the', abs_start)\n",
        "      list_abs = l_list_content[abs_start+1:abs_stop]\n",
        "      abs_result = listToString(list_abs).replace('- ', '').replace('0 ', '\\'')\n",
        "      abs_list = abs_result.split('. ')\n",
        "      print('(5) Abstract: ', end = '')\n",
        "      abstract = sen_capitalize()\n",
        "    else:\n",
        "      abs_start = l_list_content.index('a b s t r a c t')\n",
        "      abs_stop = l_list_content.index('', abs_start)\n",
        "      abs_stop2 = l_list_content.index('', abs_stop+1)\n",
        "      abs_stop3 = l_list_content.index('', abs_stop2+1)\n",
        "      if '1. introduction' == l_list_content[abs_stop3 -1]:\n",
        "        list_abs = list_content[abs_start+1:abs_stop2]\n",
        "        abs_result = listToString(list_abs).replace('- ', '').replace('0 ', '\\'')\n",
        "        abs_list = abs_result.split('. ')\n",
        "        print('(5) Abstract: ', end = '')\n",
        "        abstract = sen_capitalize()\n",
        "      else: \n",
        "        list_abs = l_list_content[abs_start+1:abs_stop3]\n",
        "        abs_result = listToString(list_abs).replace('- ', '').replace('0 ', '\\'').replace('0', '\\'')\n",
        "        abs_list = abs_result.split('. ')\n",
        "        print('(5) Abstract: ', end = '')\n",
        "        abstract = sen_capitalize()\n",
        "  elif 'abstract' not in l_list_content:\n",
        "    matching = [s for s in l_list_content if \"abstract\" in s]\n",
        "    for a in matching:\n",
        "      abs_start = l_list_content.index(a)\n",
        "      abs_stop = l_list_content.index('', abs_start)\n",
        "      list_abs = l_list_content[abs_start:abs_stop]\n",
        "      abs_result = listToString(list_abs).replace('- ', '')\n",
        "      abs = abs_result[10:]\n",
        "      abs_cap = abs_result[10:].capitalize()\n",
        "      if abs[0] == abs_cap[0]:\n",
        "        abs_list = abs_result.split('. ')\n",
        "        print('(5) Abstract: ', end = '')\n",
        "        abstract = sen_capitalize()\n",
        "      else:\n",
        "        abs_list = abs_result[10:].split('. ')\n",
        "        print('(5) Abstract: ', end = '')\n",
        "        abstract = sen_capitalize()\n",
        "  print()\n",
        "\n",
        "\n",
        "sys.stdout.close()"
      ],
      "metadata": {
        "id": "wPJQPSoxAEAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PDF 파일에서 이미지를 추출하는 것은 코드들을 시도해보아도 같은 속성 오류가 나타나며 실현되지 않습니다. 경로와 들여쓰기에는 문제가 없으며 교수님께서 pdf 파일에서 사용하신 코드를 통하여 시도하더라도 되지 않습니다. 죄송합니다 ㅠㅠ "
      ],
      "metadata": {
        "id": "p-2HDEVOHSz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (7) 이미지 추출하기 - 1\n",
        "import fitz # PyMuPDF\n",
        "import io\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "filename = '/content/drive/MyDrive/Colab Notebooks/KBSI연구과제/Differentiated influences of benefit and risk perceptions on nuclear power acceptance according to acceptance levels evidence from Korea.pdf'\n",
        "\n",
        "with fitz.open(filename) as my_pdf_file:\n",
        "    for page_number in range(1, len(my_pdf_file)+1):\n",
        "        page = my_pdf_file[page_number-1]\n",
        "        images = page.getImageList()\n",
        "        if images:\n",
        "            print(f\"There are {len(images)} image/s on page number {page_number}[+]\")\n",
        "        else:\n",
        "            print(f\"There are No image/s on page number {page_number}[!]\")\n",
        "        for image_number, image in enumerate(page.getImageList(), start=1):\n",
        "            xref_value = image[0]\n",
        "            base_image = my_pdf_file.extractImage(xref_value)\n",
        "            image_bytes = base_image[\"image\"]\n",
        "            ext = base_image[\"ext\"]\n",
        "            image = Image.open(io.BytesIO(image_bytes))\n",
        "            image.save(open(f\"Page{page_number}Image{image_number}.{ext}\", \"wb\"))"
      ],
      "metadata": {
        "id": "8Z51ctYyDsSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (7) 이미지 추출하기 - 2\n",
        "import fitz\n",
        "import io\n",
        "from PIL import Image\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/KBSI연구과제/Differentiated influences of benefit and risk perceptions on nuclear power acceptance according to acceptance levels evidence from Korea.pdf'\n",
        "open_file = fitz.open(file_path)\n",
        "\n",
        "for page_number in range(len(open_file)):\n",
        "  page = pdf_file[page_number]\n",
        "  list_image = page.getImageList()\n",
        "  if list_image:\n",
        "    print(f\"{len(list_image)} images found on page {page_number}\")\n",
        "  else:\n",
        "    print(\"No images found on page\", page_number)\n",
        "for image_number, img in enumerate(page.getImageList(), start=1):\n",
        "   xref = img[0]\n",
        "   image_base = pdf_file.extractImage(xref)\n",
        "   bytes_image = image_base[\"image\"]\n",
        "   ext_image = base_image[\"ext\"]"
      ],
      "metadata": {
        "id": "EoxijRLcDsb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (7) 이미지 추출하기 - 3\n",
        "import fitz\n",
        "import io\n",
        "from PIL import Image\n",
        "  \n",
        "\n",
        "file = '/content/drive/MyDrive/Colab Notebooks/KBSI연구과제/Differentiated influences of benefit and risk perceptions on nuclear power acceptance according to acceptance levels evidence from Korea.pdf'\n",
        "\n",
        "pdf_file = fitz.open(file)\n",
        "for page_index in range(len(pdf_file)):\n",
        "    page = pdf_file[page_index]\n",
        "    image_list = page.getImageList()\n",
        "    if image_list:\n",
        "        print(f\"[+] Found a total of {len(image_list)} images in page {page_index}\")\n",
        "    else:\n",
        "        print(\"[!] No images found on page\", page_index)\n",
        "    for image_index, img in enumerate(page.getImageList(), start=1):\n",
        "        xref = img[0]\n",
        "        base_image = pdf_file.extractImage(xref)\n",
        "        image_bytes = base_image[\"image\"]\n",
        "        image_ext = base_image[\"ext\"]"
      ],
      "metadata": {
        "id": "wzVszOU6C8sq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (7) 이미지 추출하기 - 4\n",
        "import PyPDF2\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    input1 = PyPDF2.PdfFileReader(open('/content/drive/MyDrive/Colab Notebooks/KBSI연구과제/Differentiated influences of benefit and risk perceptions on nuclear power acceptance according to acceptance levels evidence from Korea.pdf', \"rb\"))\n",
        "    page0 = input1.getPage(0)\n",
        "    xObject = page0['/Resources']['/XObject'].getObject()\n",
        "    for obj in xObject:\n",
        "        if xObject[obj]['/Subtype'] == '/Image':\n",
        "            size = (xObject[obj]['/Width'], xObject[obj]['/Height'])\n",
        "            data = xObject[obj].getData()\n",
        "            if xObject[obj]['/ColorSpace'] == '/DeviceRGB':\n",
        "                mode = \"RGB\"\n",
        "            else:\n",
        "                mode = \"P\"\n",
        "            if xObject[obj]['/Filter'] == '/FlateDecode':\n",
        "                img = Image.frombytes(mode, size, data)\n",
        "                img.save(obj[1:] + \".png\")\n",
        "            elif xObject[obj]['/Filter'] == '/DCTDecode':\n",
        "                img = open(obj[1:] + \".jpg\", \"wb\")\n",
        "                img.write(data)\n",
        "                img.close()\n",
        "            elif xObject[obj]['/Filter'] == '/JPXDecode':\n",
        "                img = open(obj[1:] + \".jp2\", \"wb\")\n",
        "                img.write(data)\n",
        "                img.close()"
      ],
      "metadata": {
        "id": "jCgTBEFfDMt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (7) 이미지 추출하기 - 5\n",
        "import fitz\n",
        "doc = fitz.open('/content/drive/MyDrive/Colab Notebooks/KBSI연구과제/Differentiated influences of benefit and risk perceptions on nuclear power acceptance according to acceptance levels evidence from Korea.pdf')\n",
        "for i in range(len(doc)):\n",
        "    for img in doc.getPageImageList(i):\n",
        "        xref = img[0]\n",
        "        pix = fitz.Pixmap(doc, xref)\n",
        "        if pix.n < 5:       \n",
        "            pix.writePNG(\"p%s-%s.png\" % (i, xref))\n",
        "        else:               \n",
        "            pix1 = fitz.Pixmap(fitz.csRGB, pix)\n",
        "            pix1.writePNG(\"p%s-%s.png\" % (i, xref))\n",
        "            pix1 = None\n",
        "        pix = None"
      ],
      "metadata": {
        "id": "hnPT0xkODXGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (7) 이미지 추출하기 - 6\n",
        "from tika import parser\n",
        "from PyPDF2 import PdfFileReader, PdfFileWriter\n",
        "import os\n",
        "import io\n",
        "import fitz\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "import sys\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/KBSI연구과제/Differentiated influences of benefit and risk perceptions on nuclear power acceptance according to acceptance levels evidence from Korea.pdf'\n",
        "pdf_file = fitz.open(file_path)\n",
        "\n",
        "for page_number in range(len(pdf_file)):\n",
        "  page = pdf_file[page_number]\n",
        "  list_image = page.getImageList()\n",
        "  if list_image:\n",
        "    print(f'{len(list_image)} images foun on page {page_number}')\n",
        "  else:\n",
        "    print('No images found in page', page_number)\n",
        "    for image_number, img in enumerate(page.getImageList(), start = 1):\n",
        "      xref = img[0]\n",
        "      image_base = pdf_file.extractImage(xref)\n",
        "      bytes_image = image_base['image']\n",
        "      ext_image = image_base['ext']\n",
        "doc = fitz.open(file_path)\n",
        "for i in range(len(doc)):\n",
        "  for img in doc.getPageImageList(i):\n",
        "    xref = img[0]\n",
        "    pix = fitz.Pixmap(doc, xref)\n",
        "    if pix.n < 5:\n",
        "      pix.writePNG()('p%s-%s.png'%(i, xref))\n",
        "    else:\n",
        "      pix1 = fitz.Pixmap(fitz.csRGB, pix)\n",
        "      pix.writePNG()('p%s-%s.png'%(i, xref))\n",
        "      pix1 = None\n",
        "    pix1 = None"
      ],
      "metadata": {
        "id": "K3pA8BgGthr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 출처\n",
        "\n",
        "# (1): https://software-creator.tistory.com/32\n",
        "# (2): https://www.debuggex.com/cheatsheet/regex/python\n",
        "# (3): https://regexr.com/\n",
        "# (4): https://ponyozzang.tistory.com/279\n",
        "# (5): https://wikidocs.net/4309\n",
        "# (6): https://python.bakyeono.net/chapter-11-2.html\n",
        "# (7): https://joyfuls.tistory.com/38\n",
        "# (8): http://aispiration.com/nlp2/regex-index.html\n",
        "# (9): https://wikidocs.net/21703\n",
        "# (10): https://velog.io/@ash3767/python-%EC%A0%95%EA%B7%9C%EC%8B%9D\n",
        "# (11): https://devkingdom.tistory.com/131\n",
        "# (12): http://pythonstudy.xyz/python/article/401-%EC%A0%95%EA%B7%9C-%ED%91%9C%ED%98%84%EC%8B%9D-Regex\n",
        "# (13): https://niceman.tistory.com/156\n",
        "# (14): https://fhaktj8-18.tistory.com/entry/%ED%8C%8C%EC%9D%B4%EC%8D%AC-%EC%A0%95%EA%B7%9C%ED%91%9C%ED%98%84%EC%8B%9D-%EA%B8%B0%EC%B4%88%EC%99%80-%EC%98%88%EC%A0%9C-%EC%82%B4%ED%8E%B4%EB%B3%B4%EA%B8%B0\n",
        "# (15): https://m.blog.naver.com/raspmldlgycchang8753/222065682333\n",
        "# (16): https://www.tandfonline.com/action/journalInformation?journalCode=uesb20\n",
        "# (17): https://nachwon.github.io/regular-expressions/\n",
        "# (18): https://whitewing4139.tistory.com/167\n",
        "# (19): https://cholol.tistory.com/457\n",
        "# (20): http://daplus.net/regex-%EB%AC%B8%EC%9E%90%EC%97%B4-%EB%82%B4%EC%97%90%EC%84%9C-url%EC%9D%84-%EC%B0%BE%EA%B8%B0%EC%9C%84%ED%95%9C-%EC%A0%95%EA%B7%9C%EC%8B%9D/\n",
        "# (21): https://coding-grandpa.tistory.com/11\n",
        "# (22): https://velog.io/@cha-suyeon/Python-%ED%8C%8C%EC%9D%B4%EC%8D%AC-%EC%BD%94%EB%94%A9-%EB%8F%84%EC%9E%A5-43.6-%EC%8B%AC%EC%82%AC%EB%AC%B8%EC%A0%9C-URL-%EA%B2%80%EC%82%AC%ED%95%98%EA%B8%B0\n",
        "# (23): https://wikidocs.net/46744\n",
        "# (24): https://greeksharifa.github.io/%EC%A0%95%EA%B7%9C%ED%91%9C%ED%98%84%EC%8B%9D(re)/2018/08/04/regex-usage-05-intermediate/\n",
        "# (25): https://moondol-ai.tistory.com/432\n",
        "# (26): https://gocoding.org/ko/program-to-check-whether-the-given-string-contains-url-using-python/\n",
        "# (27): https://wikidocs.net/142390\n",
        "# (28): https://cyber0946.tistory.com/m/59\n",
        "# (29): https://sancs.tistory.com/21\n",
        "# (30): https://www.litcoder.com/?p=2484\n",
        "# (31): https://gent.tistory.com/415\n",
        "# (32): https://www.delftstack.com/ko/howto/python/how-to-remove-last-character-from-string-in-python/\n",
        "# (33): https://lapina.tistory.com/108\n",
        "# (34): https://ooyoung.tistory.com/78\n",
        "# (35): https://iygames.tistory.com/15\n",
        "# (36): https://weeklyit.code.blog/2019/12/20/2019-12%EC%9B%94-2%EC%A3%BC-python%EC%9C%BC%EB%A1%9C-pdf-%ED%85%8C%EC%9D%B4%EB%B8%94-%EC%B6%94%EC%B6%9C%ED%95%98%EA%B8%B0/\n",
        "# (37): https://www.delftstack.com/ko/howto/python/python-capitalize-first-letter/\n",
        "# (38): https://lapina.tistory.com/108\n",
        "# (39): https://engineer-mole.tistory.com/238\n",
        "# (40): https://www.delftstack.com/ko/howto/python/remove-certain-characters-from-string-python/\n",
        "# (41): https://dojang.io/mod/page/view.php?id=1332\n",
        "# (42): http://daplus.net/python-%EB%AC%B8%EC%9E%90%EC%97%B4-%EB%AA%A9%EB%A1%9D%EC%97%90%EC%84%9C-%EB%B9%88-%EB%AC%B8%EC%9E%90%EC%97%B4-%EC%A0%9C%EA%B1%B0/\n",
        "# (43): https://jobc.tistory.com/145\n",
        "# (44): https://infinitt.tistory.com/117\n",
        "# (45): https://www.youtube.com/watch?v=hN_PVk6Guf8\n",
        "# (46): https://yganalyst.github.io/pythonic/memo_16_except/\n",
        "# (47): https://opac.tistory.com/3\n",
        "# (48): https://lapina.tistory.com/81\n",
        "# (49): https://stackoverflow.com/questions/2693820/extract-images-from-pdf-without-resampling-in-python\n",
        "# (50): https://sosoeasy.tistory.com/331\n",
        "# (51): https://wikidocs.net/77904\n",
        "# (52): https://stackoverflow.com/questions/52244164/importing-pyautogui-in-ubuntu-throwing-keyerror-display\n",
        "# (53): https://stackoverflow.com/questions/71531356/what-does-attributeerror-nonetype-object-has-no-attribute-find-all-mean-i\n",
        "# (54): https://codechacha.com/ko/python-print-dict/\n",
        "# (55): https://wikidocs.net/2858\n",
        "# (56): https://stackoverflow.com/questions/22627402/why-is-name-node-not-defined\n",
        "# (57): https://stackoverflow.com/questions/2462566/python-break-outside-loop\n",
        "# (58): https://jellyho.com/blog/96/\n",
        "# (59): https://lapina.tistory.com/108\n",
        "# (60): https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=sw4r&logNo=221504968896\n",
        "# (61): https://ai-hyu.com/python-%ED%8C%8C%EC%9D%B4%EC%8D%AC-%EB%A6%AC%EC%8A%A4%ED%8A%B8-%EC%9D%B8%EB%8D%B1%EC%8A%A4-%EC%97%AC%EB%9F%AC%EA%B0%9C-%EC%B0%BE%EA%B8%B0/\n",
        "# (62): https://tali.tistory.com/1285\n",
        "# (63): https://hashcode.co.kr/questions/1010/%EB%AC%B8%EC%9E%90%EC%97%B4%EC%9D%84-%EC%A0%80%EC%9E%A5%ED%95%98%EB%8A%94-%EB%A6%AC%EC%8A%A4%ED%8A%B8%EA%B0%80-%ED%8A%B9%EC%A0%95-%EB%AC%B8%EC%9E%90%EC%97%B4%EC%9D%84-%ED%8F%AC%ED%95%A8%ED%95%98%EB%8A%94%EC%A7%80-%EC%95%8C%EC%95%84%EB%82%B4%EB%A0%A4%EA%B3%A0-%ED%95%A9%EB%8B%88%EB%8B%A4\n",
        "# (64): https://wikidocs.net/16040\n",
        "# (65): https://dev-note-97.tistory.com/62\n",
        "# (66): https://redcow77.tistory.com/361\n",
        "# (67): https://medium.com/@ohc43312/%ED%8C%8C%EC%9D%B4%EC%8D%AC-%EB%8C%80%EC%86%8C%EB%AC%B8%EC%9E%90-%EA%B4%80%EB%A6%AC-upper-lower-isupper-islower-89aea6bd5c63\n",
        "# (68): https://elvanov.com/1618\n",
        "# (69): https://she11.tistory.com/62\n",
        "# (70): https://jimmy-ai.tistory.com/40\n",
        "# (71): https://jimmy-ai.tistory.com/40\n",
        "# (72): https://wikidocs.net/92095\n",
        "# (73): https://wikidocs.net/92112\n",
        "# (74): https://jimmy-ai.tistory.com/24\n",
        "# (75): https://kamang-it.tistory.com/entry/PIL-%EB%8B%A4%EC%9A%B4%EB%A1%9C%EB%93%9C-%ED%95%98%EA%B8%B0image-%EB%9D%BC%EC%9D%B4%EB%B8%8C%EB%9F%AC%EB%A6%AC-%EB%8B%A4%EC%9A%B4%EB%A1%9C%EB%93%9C-%ED%95%98%EA%B8%B0\n",
        "# (76): https://damio.tistory.com/50\n",
        "# (77): https://coozplz.me/category/coozplzs-category/programming/python/\n",
        "# (78): https://coozplz.me/2015/08/28/pythonmagick%EA%B3%BC-pil%EC%9D%84-%EC%9D%B4%EC%9A%A9%ED%95%9C-pdf%EC%97%90%EC%84%9C-%EC%9D%B4%EB%AF%B8%EC%A7%80-%EC%B6%94%EC%B6%9C/\n",
        "# (79): https://codesample-factory.tistory.com/1350\n",
        "# (80): https://blog.naver.com/PostView.naver?blogId=ycl2k&logNo=222299093679&parentCategoryNo=15&categoryNo=&viewDate=&isShowPopularPosts=true&from=search\n",
        "# (81): https://stackoverflow.com/questions/68422506/python-attributeerror-page-object-has-no-attribute-getcontents\n",
        "# (82): https://stackoverflow.com/questions/2693820/extract-images-from-pdf-without-resampling-in-python\n",
        "# (83): https://www.codeit.kr/community/threads/24417\n",
        "# (84): https://opentutorials.org/module/2980/17644\n",
        "# (85): https://mabb.tistory.com/102\n",
        "# (86): https://codedragon.tistory.com/9342\n",
        "# (87): https://hashcode.co.kr/questions/10332/%ED%8C%8C%EC%9D%B4%EC%8D%AC-csv-%EA%B4%80%EB%A0%A8-%EC%A7%88%EB%AC%B8-valueerror-io-operation-on-closed-file\n",
        "# (88): https://homy.tistory.com/53\n",
        "# (89): https://codetorial.net/tips_and_examples/save_print_output.html\n",
        "# (90): https://bskyvision.com/846\n",
        "# (91): https://daewoonginfo.blogspot.com/2020/06/python-matplotlib.html\n",
        "# (92): 노승국 교수님께서 KBSI 연구와 관련하여 보내주신 PDF 파일인 연구장비 공동활용 논문 주요데이터 추출 및 키워드 분석 체계 구축\n",
        "# (93): 초보자를 위한 파이썬 200제, 장삼용, 정보문화사, 2017.\n",
        "# (94): 파이썬 입문 (예제 중심), 황재호, 인포앤북, 2020.\n",
        "# (95): 파이썬 딥러닝 머신러닝 입문, 오승환, 정보문화사, 2021.\n",
        "# (96): 두근두근 파이썬, 천인국, 생능출판, 2017.\n",
        "# (97): 파이썬 텍스트 마이닝 완벽 가이드, 박상언, 강주영, 위키북스, 2022.\n",
        "# (98): 혼자 공부하는 파이썬, 윤인성, 한빛미디어, 2022.\n",
        "# (99): Do it! 점프 투 파이썬, 박응용, 이지스리퍼블리싱, 2019.\n",
        "# (100): 파이썬 생활밀찰형 프로젝트, 김효실, 로드북, 2021.\n"
      ],
      "metadata": {
        "id": "N4BpXFXiQJr2"
      }
    }
  ]
}